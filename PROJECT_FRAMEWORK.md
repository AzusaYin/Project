
# ============================================================================
# File: app/admin_docs.py
# ============================================================================
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from pathlib import Path
import shutil, json, time, os, tempfile
import asyncio
from typing import List, Dict, Any

from .settings import settings
from .rag import ingest_corpus
from .security import require_bearer
from .ingest_manager import start as ingest_start, cancel as ingest_cancel, status as ingest_status


router = APIRouter(prefix="/docs", tags=["docs"])

DOCS_DIR = Path(settings.docs_dir)
STATUS_PATH = Path("data/status.json")
TMP_DIR = Path("data/tmp_uploads")
TMP_DIR.mkdir(parents=True, exist_ok=True)
DOCS_DIR.mkdir(parents=True, exist_ok=True)

def _write_status(payload: Dict[str, Any]) -> None:
    STATUS_PATH.parent.mkdir(parents=True, exist_ok=True)
    data = json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")
    # åŸå­å†™ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå† replace
    with tempfile.NamedTemporaryFile(dir=str(STATUS_PATH.parent), delete=False) as tf:
        tf.write(data)
        tmpname = tf.name
    os.replace(tmpname, STATUS_PATH)

def _reindex_job(note: str):
    _write_status({"status": "indexing", "note": note, "start_ts": int(time.time())})
    try:
        # ingest_corpus å¯ä»¥æ˜¯åŒæ­¥å‡½æ•°ï¼›è‹¥ä½ å®ç°çš„æ˜¯ asyncï¼Œå¯åœ¨è¿™é‡Œç”¨ anyio.run è°ƒç”¨
        if asyncio.iscoroutinefunction(ingest_corpus):
            import anyio
            anyio.run(ingest_corpus)
        else:
            ingest_corpus()
        _write_status({"status": "ready", "note": note, "last_built": int(time.time())})
    except Exception as e:
        _write_status({"status": "error", "note": f"{note}: {e}", "ts": int(time.time())})

@router.get("/status", dependencies=[Depends(require_bearer)])
def get_status():
    if STATUS_PATH.exists():
        try:
            return json.loads(STATUS_PATH.read_text(encoding="utf-8"))
        except Exception:
            return {"status": "unknown"}
    return {"status": "ready"}

@router.get("/list", dependencies=[Depends(require_bearer)])
def list_docs():
    items = []
    for p in sorted(list(DOCS_DIR.glob("*.md")) + list(DOCS_DIR.glob("*.markdown"))):
        items.append({
            "filename": p.name,
            "size": p.stat().st_size,
            "modified": int(p.stat().st_mtime),
        })
    return {"docs": items}

@router.post("/upload", dependencies=[Depends(require_bearer)])
async def upload_doc(file: UploadFile = File(...)):
    name = (file.filename or "").lower()
    if not (name.endswith(".md") or name.endswith(".markdown")):  # å¦‚éœ€åŒæ—¶æ”¯æŒ pdfï¼Œå¯åŠ  or name.endswith(".pdf")
        raise HTTPException(400, "Only Markdown (.md/.markdown) is supported")
    
    tmp_path = TMP_DIR / (file.filename + ".part")
    with tmp_path.open("wb") as f:
        shutil.copyfileobj(file.file, f)
    dest = DOCS_DIR / file.filename
    tmp_path.replace(dest)  # åŸå­ç§»åŠ¨

    # ===== çŠ¶æ€ï¼šindexing =====
    _write_status({"status": "indexing", "note": f"uploaded: {file.filename}", "start_ts": int(time.time())})
   
    # â€”â€” å…³é”®ï¼šåŒæ­¥é‡å»ºï¼ˆé˜»å¡ç›´åˆ°å®Œæˆï¼‰â€”â€”
    # è‹¥ ingest_corpus æ˜¯ CPU/IO å¯†é›†ï¼ŒåŒæ­¥è°ƒç”¨ä¼šå¡ä½ event loopï¼›
    # ç”¨ asyncio.to_thread è®©å…¶åœ¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œä½†è¿™é‡Œä¾ç„¶â€œç­‰å®ƒå®Œæˆå†è¿”å›â€ï¼Œä½“éªŒç­‰ä»·äºé˜»å¡å¼ã€‚
    await asyncio.to_thread(ingest_corpus, settings.docs_dir, settings.index_dir)

    # å…³é”®ï¼šå¤±æ•ˆå†…å­˜ç´¢å¼•ç¼“å­˜
    from . import main as _main
    _main._index = None
    _main._embedder = None

    # ===== çŠ¶æ€ï¼šready =====
    _write_status({"status": "ready", "note": f"uploaded: {file.filename}", "last_built": int(time.time())})
    return {"ok": True, "message": f"{file.filename} uploaded. Index rebuilt (hot-loaded)."}

@router.delete("/{filename}", dependencies=[Depends(require_bearer)])
async def delete_doc(filename: str):
    # åŸºç¡€æ ¡éªŒï¼Œé˜»æ­¢è·¯å¾„ç©¿è¶Š
    if "/" in filename or "\\" in filename:
        raise HTTPException(400, "Bad filename")

    # å…è®¸çœç•¥åç¼€ã€å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
    candidates = [
        DOCS_DIR / filename,
        DOCS_DIR / (filename if filename.lower().endswith(".md") else filename + ".md"),
        DOCS_DIR / (filename if filename.lower().endswith(".markdown") else filename + ".markdown"),
    ]

    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨ç›®å½•é‡Œåšä¸€æ¬¡â€œå¤§å°å†™ä¸æ•æ„Ÿâ€/è¿‘ä¼¼åŒ¹é…
    path = next((p for p in candidates if p.exists()), None)
    if path is None:
        low = filename.lower()
        for p in DOCS_DIR.glob("*"):
            if p.name.lower() == low or p.name.lower() == (low + ".md") or p.name.lower() == (low + ".markdown"):
                path = p
                break

    # èƒ½æ‰¾åˆ°å°±åˆ ï¼Œæ‰¾ä¸åˆ°ä¹Ÿç»§ç»­é‡å»ºï¼ˆä¿è¯ç´¢å¼•ä¸ç£ç›˜ä¸€è‡´ï¼‰
    if path and path.exists():
        path.unlink()
    
    # â€”â€” çŠ¶æ€ï¼šindexing â€”â€”
    _write_status({"status": "indexing", "note": f"deleted: {filename}", "start_ts": int(time.time())})

    try:
        # â€”â€” åŒæ­¥é‡å»ºï¼ˆçƒ­åŠ è½½ï¼‰â€”â€”
        await asyncio.to_thread(ingest_corpus, settings.docs_dir, settings.index_dir)

        # å…³é”®ï¼šå¤±æ•ˆå†…å­˜ç´¢å¼•ç¼“å­˜
        from . import main as _main
        _main._index = None
        _main._embedder = None

        # â€”â€” çŠ¶æ€ï¼šready â€”â€”
        _write_status({"status": "ready", "note": f"deleted: {filename}", "last_built": int(time.time())})
        return {"ok": True, "message": f"{filename} deleted (if existed). Index rebuilt (hot-loaded)."}
    except Exception as e:
        # â€”â€” çŠ¶æ€ï¼šerrorï¼ˆå¯åœ¨å‰ç«¯æç¤ºï¼‰â€”â€”
        _write_status({"status": "error", "note": f"deleted: {filename}: {e}", "ts": int(time.time())})
        raise HTTPException(status_code=500, detail=f"Reindex failed: {e}")
        
@router.post("/cancel", dependencies=[Depends(require_bearer)])
def cancel_reindex():
    killed = ingest_cancel()
    return {"ok": True, "killed": killed}

# ============================================================================
# File: app/ingest_manager.py
# ============================================================================
from __future__ import annotations
from multiprocessing import Process
from pathlib import Path
import json, os, tempfile, time, traceback
from typing import Dict, Any, Optional
from .rag import ingest_corpus
from .settings import settings

STATUS_PATH = Path("data/status.json")
STATUS_PATH.parent.mkdir(parents=True, exist_ok=True)

_proc: Optional[Process] = None
_note: str = ""
_start_ts: Optional[int] = None

def _write_status(payload: Dict[str, Any]) -> None:
    data = json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")
    with tempfile.NamedTemporaryFile(dir=str(STATUS_PATH.parent), delete=False) as tf:
        tf.write(data)
        tmpname = tf.name
    os.replace(tmpname, STATUS_PATH)

def _target(note: str):
    # å­è¿›ç¨‹é‡Œæ‰§è¡ŒçœŸæ­£çš„é‡å»º
    _write_status({"status":"indexing", "note":note, "start_ts":int(time.time())})
    try:
        ingest_corpus(settings.docs_dir, settings.index_dir)
        _write_status({"status":"ready", "note":note, "last_built":int(time.time())})
    except Exception:
        err = traceback.format_exc()
        _write_status({"status":"error", "note":f"{note}: {err}", "ts":int(time.time())})

def start(note: str) -> bool:
    global _proc, _note, _start_ts
    if _proc is not None and _proc.is_alive():
        return False  # å·²æœ‰ä»»åŠ¡åœ¨è·‘ï¼Œè¿”å› False
    _note = note
    _start_ts = int(time.time())
    _proc = Process(target=_target, args=(note,), daemon=True)
    _proc.start()
    return True

def cancel() -> bool:
    global _proc
    if _proc is None:
        return False
    alive = _proc.is_alive()
    if alive:
        _proc.terminate()
        _proc.join(timeout=5)
        _write_status({"status":"canceled", "note":"canceled by user", "ts":int(time.time())})
    _proc = None
    return alive

def status() -> Dict[str, Any]:
    s: Dict[str, Any]
    if STATUS_PATH.exists():
        try:
            s = json.loads(STATUS_PATH.read_text(encoding="utf-8"))
        except Exception:
            s = {"status":"unknown"}
    else:
        s = {"status":"ready"}
    if _proc is not None:
        s["running"] = _proc.is_alive()
        s["pid"] = _proc.pid
    return s

# ============================================================================
# File: app/llm_client.py
# ============================================================================
import httpx
from typing import List, AsyncGenerator
from .settings import settings

# stream & non-stream are split to avoid async generator return conflicts
async def smartcare_chat_stream(messages: List[dict]) -> AsyncGenerator[str, None]:
    payload = {
        "messages": messages,
        "temperature": settings.temperature,
        "max_tokens": settings.max_tokens,
        "stream": True,
    }
    async with httpx.AsyncClient(timeout=60) as client:
        async with client.stream("POST", settings.smartcare_base_url, json=payload) as resp:
            resp.raise_for_status()
            async for line in resp.aiter_lines():
                if line:
                    yield line

async def smartcare_chat(messages: List[dict]) -> dict:
    payload = {
        "messages": messages,
        "temperature": settings.temperature,
        "max_tokens": settings.max_tokens,
        "stream": False,
    }
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(settings.smartcare_base_url, json=payload)
        r.raise_for_status()
        try:
            data = r.json()
        except Exception:
            text = r.text
            return {"choices": [{"message": {"role": "assistant", "content": text}}]}
        if isinstance(data, dict) and "choices" not in data:
            content = data.get("content") or data.get("answer") or data.get("message") or data.get("data")
            if isinstance(content, str):
                return {"choices": [{"message": {"role": "assistant", "content": content}}]}
        return data

async def smartcare_rewrite_query(messages: list[dict]) -> str:
    try:
        sys = {
            "role": "system",
            "content": (
                "You are a query rewriter. Rewrite ONLY the last user message into a standalone, context-complete question. "
                "Resolve pronouns (it/its/this/that/they/their; å®ƒ/å…¶/é€™/è©² ç­‰) to the most recent explicit entity mentioned "
                "in the conversation, especially policy/program names (e.g., Old Age Allowance, Old Age Living Allowance, LSG Subvention Manual).\n"
                "Keep the original language. Output the rewritten question text only.\n\n"
                "Examples:\n"
                "User history: What is Old Age Allowance?\n"
                "Last user: What are its eligibility requirements?\n"
                "Rewrite: What are the eligibility requirements for the Old Age Allowance?\n\n"
                "User history: ä»€éº¼æ˜¯é•·è€…ç”Ÿæ´»æ´¥è²¼ï¼Ÿ\n"
                "Last user: ç”³è«‹è¦å“ªäº›æ–‡ä»¶ï¼Ÿ\n"
                "Rewrite: é•·è€…ç”Ÿæ´»æ´¥è²¼çš„ç”³è«‹éœ€è¦å“ªäº›æ–‡ä»¶ï¼Ÿ"
            ),
        }
        payload_msgs = [sys] + messages
        data = await smartcare_chat(payload_msgs)
        text = data.get("choices", [{}])[0].get("message", {}).get("content") or ""
        return (text or "").strip() or messages[-1]["content"]
    except Exception:
        return messages[-1]["content"]

async def smartcare_translate_to_en(text: str) -> str:
    """
    å°†è¼¸å…¥ç¿»è­¯ç‚ºè‹±æ–‡ï¼ˆä¿æŒåŸæ„ã€ç”¨æ–¼æª¢ç´¢ï¼‰ï¼Œè‹¥å¤±æ•—å‰‡å›é€€åŸæ–‡ã€‚
    """
    try:
        sys = {
            "role": "system",
            "content": (
                "Translate the user's text into natural English suitable for information retrieval. "
                "Preserve the meaning. Output English only, no explanations."
            ),
        }
        data = await smartcare_chat([sys, {"role": "user", "content": text}])
        out = (data.get("choices", [{}])[0].get("message", {}) or {}).get("content", "") or ""
        return out.strip() or text
    except Exception:
        return text

# ============================================================================
# File: app/main.py
# ============================================================================
import uvicorn
import numpy as np
import re, json, time
from pathlib import Path
from typing import List, Dict, Tuple
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from .settings import settings
from .schemas import ChatRequest, ChatAnswer, IngestResponse, FeedbackIn
from .rag import Index, Embedder, hybrid_retrieve, build_prompt, format_citations, ingest_corpus
from .llm_client import smartcare_chat, smartcare_chat_stream, smartcare_translate_to_en
from .security import require_bearer
from .admin_docs import router as admin_docs_router

app = FastAPI(title="ElderlyCare HK â€” Backend")
app.include_router(admin_docs_router)

_CITE_TAG_RE = re.compile(r"\[Source\s+(\d+)\]")
_PRONOUN_PAT = re.compile(r"\b(it|its|this|that|they|their)\b|[å®ƒå…¶é€™è©²]")
_CJK_RE = re.compile(r"[\u4e00-\u9fff]")  # åŸºæœ¬æ¼¢å­—

# ç®€å•åˆ«åæ˜ å°„ï¼ˆå¯ç»§ç»­è¡¥å……ï¼‰
ALIASES = {
    "OAA": "Old Age Allowance",
    "OALA": "Old Age Living Allowance",
    "LSG": "Lump Sum Grant",
    "LSGSS": "Lump Sum Grant Subvention System",
}

# ç”¨â€œéå­—æ¯æ•°å­—â€å‰åè§†å›¾æ›¿ä»£ \bï¼Œé¿å…åœ¨ä¸­æ–‡é‡Œå¤±æ•ˆ
# (?<![A-Za-z0-9]) â€¦â€¦ (?![A-Za-z0-9])
_ENTITY_EXTRACT_RE = re.compile(
    r"(?<![A-Za-z0-9])("                                   # è‹±æ–‡æ­£å¼åç¨±ï¼ˆé¦–å­—æ¯å¤§å¯«çš„å¤šè©çŸ­èª + å¾Œç¶´ï¼‰
    r"[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,7}\s"
    r"(?:Allowance|Scheme|Program|Programme|Manual|Handbook|Guide|Guidance\s+Notes|Notes|Policy|"
    r"Ordinance|Regulation|Circular|Arrangement|Grant|Service|Subvention|System|Framework|Code|"
    r"Plan|Charter|Protocol|Directive|Guideline)s?"         # å…è¨±å¯é¸è¤‡æ•¸ s
    r")(?![A-Za-z0-9])"
    r"|(?:Old\s+Age\s+Allowance|Old\s+Age\s+Living\s+Allowance|Disability\s+Allowance|"
    r"Comprehensive\s+Social\s+Security\s+Assistance)"      # å¸¸è¦‹è‹±æ–‡å…¨å
    r"|(?:OAA|OALA|CSSA|DA|LSG|LSGSS)"                      # å¸¸è¦‹è‹±æ–‡ç¸®å¯«
    r"|(?:é•·è€…ç”Ÿæ´»æ´¥è²¼|é«˜é½¡æ´¥è²¼|è€å¹´æ´¥è²¼|å‚·æ®˜æ´¥è²¼|"
    r"ç¶œåˆç¤¾æœƒä¿éšœæ´åŠ©|è³‡åŠ©ç¦åˆ©æœå‹™|çµ±ä¸€æ’¥æ¬¾|è³‡åŠ©æ‰‹å†Š|æ’¥æ¬¾åˆ¶åº¦|"
    r"æ”¿ç­–|è¨ˆåŠƒ|æ´¥è²¼|æ‰‹å†Š|æŒ‡å¼•|é€šå‘Š|è¦ä¾‹|æ¢ä¾‹|æ–¹æ¡ˆ|åˆ¶åº¦|å®‰æ’)"  # ç¹ä¸­å¾Œç¶´/åŒç¾©è©æ“´å……
    , re.I
)

# é€™å€‹æ¯”ä¸Šé¢çš„ç¨çª„ï¼Œç”¨æ–¼ä½ çš„â€œå¯¦é«”æç¤º/æ¾„æ¸…â€æª¢æ¸¬ï¼ˆä¸éœ€è¦éå¤šå¹²æ“¾è©ï¼‰
_ENTITY_PAT = re.compile(
    r"(?<![A-Za-z0-9])("
    r"[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,6}\s"
    r"(?:Allowance|Scheme|Program|Programme|Manual|Handbook|Policy|Ordinance|Regulation|"
    r"Guideline|Circular|Grant|Service|Subvention|System)s?"
    r")(?![A-Za-z0-9])"
    r"|(?:Old\s+Age\s+Allowance|Old\s+Age\s+Living\s+Allowance|Operating\s+Subvented\s+Welfare|"
    r"LSG\s+Subvention\s+Manual|Disability\s+Allowance|Comprehensive\s+Social\s+Security\s+Assistance)"
    r"|(?:OAA|OALA|CSSA|DA|LSG|LSGSS)"
    r"|(?:é•·è€…ç”Ÿæ´»æ´¥è²¼|é«˜é½¡æ´¥è²¼|è€å¹´æ´¥è²¼|å‚·æ®˜æ´¥è²¼|è³‡åŠ©ç¦åˆ©æœå‹™|çµ±ä¸€æ’¥æ¬¾|è³‡åŠ©æ‰‹å†Š|"
    r"æ’¥æ¬¾åˆ¶åº¦|è¨ˆåŠƒ|æ´¥è²¼|æ‰‹å†Š|æŒ‡å¼•|é€šå‘Š|è¦ä¾‹|æ¢ä¾‹|åˆ¶åº¦)"
    , re.I
)

# --- Helpers for zh-Hant queries ---
def _merge_dedup_hits(h1: list[dict], h2: list[dict], k: int) -> list[dict]:
    """ä»¥ (file, page, chunk_id) å»é‡ï¼›åˆ†æ•°å–è¾ƒå¤§å€¼ï¼›ä¿ç•™æ¥æºæ ‡è®°ä¾¿äºè°ƒè¯•"""
    seen = {}
    for src, tag in ((h1, "zh"), (h2, "en")):
        for h in (src or []):
            key = (h.get("file"), h.get("page"), h.get("chunk_id"))
            score = float(h.get("score", 0))
            if key not in seen or score > seen[key]["score"]:
                seen[key] = {**h, "score": score, "src": tag}
    return sorted(seen.values(), key=lambda x: x["score"], reverse=True)[:k]

def _expand_aliases_zh(q: str) -> str:
    """æŠŠç¹ä¸­çš„ä¿—ç§°/ç®€ç§°æ‰©æˆ (A OR B OR è‹±æ–‡å)ï¼›ä½ å¯æŠŠè¡¨æ…¢æ…¢è¡¥å……èµ·æ¥"""
    table = {
        "ç”Ÿæœé‡‘": ["é«˜é½¡æ´¥è²¼", "Old Age Allowance", "OAA"],
        "ç¶œæ´": ["ç¶œåˆç¤¾æœƒä¿éšœæ´åŠ©", "Comprehensive Social Security Assistance", "CSSA"],
    }
    # å…ˆå¤„ç†ç¹ä¸­æ–‡æœ¬
    for k, vs in table.items():
        if k in q:
            q = q.replace(k, f"({ ' OR '.join([k] + vs) })")
    # å†å¥—ç”¨ä½ ç°æœ‰çš„è‹±æ–‡ç¼©å†™è¡¨
    for short, full in ALIASES.items():
        if short in q:
            q = q.replace(short, f"({short} OR {full})")
    return q

def _norm_for_entity(s: str) -> str:
    # å…¨è§’->åŠè§’ + å»æ‰å¤šé¤˜ç©ºç™½
    out = []
    for ch in s:
        code = ord(ch)
        if code == 0x3000: code = 0x20
        elif 0xFF01 <= code <= 0xFF5E: code -= 0xFEE0
        out.append(chr(code))
    return re.sub(r"\s+", " ", "".join(out)).strip()

def _extract_focus_phrase(s: str) -> str | None:
    s = _norm_for_entity(s or "")
    # ä¹¦åå·ä¼˜å…ˆï¼šå¦‚ã€Šæ´¥è²¼åŠæœå‹™å”è­°ã€‹
    m = re.search(r"ã€Š(.+?)ã€‹", s)
    if m: return m.group(1).strip()
    # é€€åŒ–ï¼šè¿ç»­å¤§å†™å¼€å¤´è¯ + å…³é”®å°¾è¯ï¼ˆAccounts/Allowance/Manual/...ï¼‰
    m = re.search(r"([A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,6}\s(?:Accounts?|Allowance|Manual|Programme|Scheme|Policy|System|Subvention))", s)
    return m.group(1).strip() if m else None

def _boost_by_phrase(contexts: list[dict], phrase: str | None, boost: float = 0.35) -> list[dict]:
    if not phrase: 
        return contexts
    for c in contexts:
        t = (c.get("text") 
             or (c.get("meta") or {}).get("text") 
             or "")
        if t and phrase in t:
            c["score"] = float(c.get("score", 0.0)) + boost
    return sorted(contexts, key=lambda x: float(x.get("score", 0.0)), reverse=True)

def _extract_entities_from_text(text: str) -> list[str]:
    if not text: 
        return []
    return [m.group(0).strip() for m in _ENTITY_EXTRACT_RE.finditer(text)]

def _suggest_entities_for(query: str, idx: "Index", emb: "Embedder", top_k: int = 5) -> list[str]:
    """
    ç”¨ç•¶å‰æŸ¥è©¢åœ¨ç´¢å¼•è£¡åšä¸€æ¬¡è¼•é‡æª¢ç´¢ï¼Œå¾å‘½ä¸­çš„ chunk æ–‡æœ¬ä¸­æŠ½å–å¯¦é«”åï¼ŒæŒ‰é »æ¬¡å»é‡æ’åºè¿”å›ã€‚
    """
    try:
        contexts = hybrid_retrieve(query, idx, emb, k=20, soft=True)
    except Exception:
        return []
    freq: dict[str, int] = {}
    for c in contexts:
        t = c["meta"].get("text") or ""
        for ent in _extract_entities_from_text(t):
            # åˆä½µå¤§å°å¯«/ç©ºç™½å·®ç•°
            key = re.sub(r"\s+", " ", ent).strip()
            freq[key] = freq.get(key, 0) + 1
    # æ’åº + å»æ‰éæ–¼ç± çµ±çš„è©
    bad = {"Allowance","Scheme","Manual","Programme","Grant","Service","Subvention","System","Policy","è¨ˆåŠƒ","æ´¥è²¼","æ‰‹å†Š","æ”¿ç­–"}
    items = [e for e,_ in sorted(freq.items(), key=lambda kv: kv[1], reverse=True) if e not in bad]
    # ä¿ç•™ä¸åŒå‰ç¶´çš„å‰ top_k å€‹
    out = []
    seen_lower = set()
    for e in items:
        low = e.lower()
        if low in seen_lower: 
            continue
        seen_lower.add(low)
        out.append(e)
        if len(out) >= top_k:
            break
    return out

def _looks_cjk(s: str) -> bool:
    return bool(_CJK_RE.search(s or ""))
# è½»é‡å¯å‘å¼å‚æ•°
_MIN_TOKENS = 3
_GENERIC_Q_RE = re.compile(
    r"^(what|how|why|tell me|can you|could you|explain|give me|i want to know|èªªèªª|ä»‹ç´¹|è§£é‡‹|è«‹è¬›è¬›|æˆ‘æƒ³çŸ¥é“)\b",
    re.I,
)

def _looks_specific(q: str, contexts: list[dict]) -> bool:
    """
    è¿”å› True è¡¨ç¤ºâ€œè¿™ä¸ªæŸ¥è¯¢å·²ç»è¶³å¤Ÿå…·ä½“â€ï¼Œä¸è¦å†è§¦å‘æ¾„æ¸…ã€‚
    åˆ¤æ®ï¼š
      - å«ã€Šã€‹ä¹¦åå·ï¼ˆé€šå¸¸æ˜¯ç¡®æŒ‡æ ‡é¢˜ï¼‰
      - å‘½ä¸­ä½ å®šä¹‰çš„å®ä½“æ­£åˆ™ï¼ˆæ”¿ç­–/æ´¥è´´åç­‰ï¼‰
      - åœ¨ topN å€™é€‰æ–‡æœ¬é‡Œå‡ºç°äº†åŸæ ·çŸ­è¯­ï¼ˆä¸¥æ ¼åŒ…å«ï¼‰
    """
    qn = _norm_for_entity(q)
    if "ã€Š" in qn and "ã€‹" in qn: return True
    if _ENTITY_PAT.search(qn):    return True
    inner = None
    m = re.search(r"ã€Š(.+?)ã€‹", qn)
    if m: inner = m.group(1).strip()
    phrase = (inner or qn).strip()
    if phrase:
        for c in contexts[:10]:
            t = (c.get("text") or (c.get("meta") or {}).get("text") or "")
            if t and phrase in t:
                return True
    return False

def _tokenize_simple(s: str) -> list[str]:
    return [t for t in re.findall(r"\w+|[\u4e00-\u9fff]", s or "") if t.strip()]

def _is_ambiguous_heuristic(q: str) -> bool:
    q = (q or "").strip()
    if not q:
        return True
    toks = _tokenize_simple(q)
    if len(toks) <= _MIN_TOKENS:
        return True
    uniq_ratio = len(set(toks)) / max(1, len(toks))
    if uniq_ratio < 0.5:
        return True
    has_named = bool(re.search(r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,4})\b", q))
    if _GENERIC_Q_RE.search(q) and not has_named:
        return True
    return False

# è¯­ä¹‰æ¨¡æ¿ï¼ˆæå°é›†åˆï¼Œæ•æ‰â€œæ³›é—®/æ‰©å†™/è§£é‡Šä¸€ä¸‹â€è¿™ç±»ï¼‰
_GENERIC_TEMPLATES = [
    "Tell me about it",
    "Tell me about this",
    "What is it",
    "Explain this",
    "Give me details",
    "I want to know more",
    "What about it",
    "è«‹ä»‹ç´¹ä¸€ä¸‹",
    "é€™æ˜¯ä»€éº¼",
    "èªªèªªçœ‹",
]

_GENERIC_EMB: np.ndarray | None = None  # æ‡’åŠ è½½ç¼“å­˜



def _ensure_generic_emb() -> np.ndarray:
    global _GENERIC_EMB
    if _GENERIC_EMB is None:
        emb = get_embedder()  # å¤ç”¨ç°æœ‰çš„ SentenceTransformerï¼ˆå·²åš normalizeï¼‰
        _GENERIC_EMB = emb.encode(_GENERIC_TEMPLATES)
        if _GENERIC_EMB.ndim == 1:
            _GENERIC_EMB = _GENERIC_EMB.reshape(1, -1)
    return _GENERIC_EMB

def _is_ambiguous_semantic(q: str, thr: float = 0.78) -> bool:
    q = (q or "").strip()
    if not q:
        return True
    emb = get_embedder()
    qe = emb.encode([q])[0]  # å·²å½’ä¸€åŒ–
    G = _ensure_generic_emb()  # (m, d)
    sims = (G @ qe)  # ä½™å¼¦ç›¸ä¼¼åº¦
    return float(np.max(sims)) >= thr

def _should_clarify_smart(user_query: str) -> bool:
    # Aï¼šå¯å‘å¼å…ˆåˆ¤
    if _is_ambiguous_heuristic(user_query):
        return True
    # Bï¼šä¸â€œæ³›é—®æ¨¡æ¿â€ç›¸ä¼¼åˆ™åˆ¤æ¨¡ç³Š
    if _is_ambiguous_semantic(user_query):
        return True
    return False

def _clarify_question(user_query: str, lang: str | None) -> str:
    """
    å½“æ£€æµ‹åˆ°ç”¨æˆ·é—®é¢˜æ¨¡ç³Šæ—¶ï¼Œç”Ÿæˆä¸€æ¡è¿½é—®å¥ï¼Œç”¨äºæç¤ºç”¨æˆ·å…·ä½“åŒ–é—®é¢˜ã€‚
    """
    if not user_query:
        user_query = "your question"

    # ä¸­æ–‡ç•Œé¢
    if lang == "zh-Hant":
        return (
            f"ä½ çš„å•é¡Œï¼ˆã€Œ{user_query}ã€ï¼‰ç›®å‰ç¯„åœéå¤§ã€‚"
            "æˆ‘åœ¨ç›¸é—œçš„å®˜æ–¹æ–‡æª”ä¸­æ‰¾ä¸åˆ°æœ‰é—œè©²ä¸»é¡Œçš„ä¿¡æ¯ã€‚"
        )

    # è‹±æ–‡ç•Œé¢
    return (
        f"Your question (â€œ{user_query}â€) is a bit broad. "
        "I couldn't find any information on that topic in documents related to elderly care."
    )

def _clarify_question_smart(user_query: str, lang: str | None, idx: "Index", emb: "Embedder") -> str:
    q = (user_query or "").strip()
    # å…ˆåšå€™é¸ï¼šæ¯”å¦‚ "allowance", "policy", "scheme", "æ´¥è²¼", "æ”¿ç­–"
    keywords = ["allowance", "policy", "scheme", "manual", "programme", "æ´¥è²¼", "æ”¿ç­–", "è¨ˆåŠƒ", "æ‰‹å†Š"]
    need_list = any(kw in q.lower() for kw in keywords) or _is_ambiguous_semantic(q)

    if need_list:
        cands = _suggest_entities_for(q, idx, emb, top_k=5)
        if cands:
            if lang == "zh-Hant":
                opts = "ã€".join(cands[:4])
                return f"ä½ çš„å•é¡Œè¼ƒç‚ºç± çµ±ï¼ˆã€Œ{q}ã€ï¼‰ã€‚ä½ æ˜¯åœ¨å• {opts}ï¼Œé‚„æ˜¯å…¶ä»–ï¼Ÿ"
            else:
                opts = ", ".join(cands[:4])
                return f'Your question (â€œ{q}â€) is a bit broad. Are you asking about {opts}, or something else?'

    # å€™é¸ç©ºæ™‚ï¼Œå›é€€åˆ°åŸä¾†çš„é€šç”¨æç¤º
    return _clarify_question(user_query, lang)

def _expand_aliases(text: str) -> str:
    out = text
    for k, v in ALIASES.items():
        out = re.sub(rf"\b{k}\b", v, out, flags=re.I)
    return out

def _guess_entity_from_history(msgs: list[dict]) -> str | None:
    """å¾å°è©±æ­·å²ä¸­çŒœæ¸¬æœ€è¿‘å‡ºç¾çš„æ˜ç¢ºæ”¿ç­–/æ´¥è²¼åï¼ˆæ”¯æ´ç¹ä¸­èˆ‡å…¨è§’ï¼‰"""
    for m in reversed(msgs):
        txt = _expand_aliases((m.get("content") or "").strip())
        if not txt:
            continue
        # ğŸ”¹åœ¨åŒ¹é…å‰åšæ­£è¦åŒ–
        normed = _norm_for_entity(txt)
        hit = _ENTITY_PAT.search(normed)
        if hit:
            return hit.group(0)

    # è‹¥ä»æœªå‘½ä¸­ï¼Œé€€å›é¦–è¼ªè¨Šæ¯å†è©¦
    for m in msgs:
        txt = _expand_aliases((m.get("content") or "").strip())
        normed = _norm_for_entity(txt)
        hit = _ENTITY_PAT.search(normed)
        if hit:
            return hit.group(0)
    return None

_FEEDBACK_DIR = Path("data/feedback")
_FEEDBACK_DIR.mkdir(parents=True, exist_ok=True)
_FEEDBACK_PATH = _FEEDBACK_DIR / "feedback.jsonl"
_METRICS_PATH  = _FEEDBACK_DIR / "metrics.json"   # ç”¨äºæç®€åœ¨çº¿æŒ‡æ ‡

def _append_jsonl(path: Path, obj: dict):
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _bump_metrics(label: str):
    # è¶…è¿·ä½ åœ¨çº¿æŒ‡æ ‡ï¼šç´¯è®¡ up/down æ¬¡æ•°ï¼›å¯æ‰©å±•ä¸ºåˆ†æ¡¶(ç›¸ä¼¼åº¦åŒºé—´)ç»Ÿè®¡
    m = {"up": 0, "down": 0}
    if _METRICS_PATH.exists():
        try: m = json.loads(_METRICS_PATH.read_text(encoding="utf-8"))
        except Exception: pass
    m[label] = m.get(label, 0) + 1
    _METRICS_PATH.write_text(json.dumps(m), encoding="utf-8")

@app.post("/feedback")
async def feedback_in(body: FeedbackIn, _auth=Depends(require_bearer)):
    rec = body.model_dump()
    rec["ts"] = int(time.time() * 1000)
    try:
        _append_jsonl(_FEEDBACK_PATH, rec)
        _bump_metrics(body.label)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"write feedback failed: {e}")
    return {"ok": True}

@app.middleware("http")
async def add_pna_header(request: Request, call_next):
    # è®© Chrome çš„ Private Network Access é¢„æ£€é€šè¿‡
    response: Response = await call_next(request)
    response.headers["Access-Control-Allow-Private-Network"] = "true"
    return response

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

_index: Index | None = None
_embedder: Embedder | None = None

def _extract_used_indices(answer_text: str, max_k: int) -> List[int]:
    """ä»ç­”æ¡ˆæ­£æ–‡é‡Œæå–å®é™…è¢«ä½¿ç”¨çš„ [Source n]ï¼Œå»é‡å¹¶æŒ‰é¦–æ¬¡å‡ºç°é¡ºåºè¿”å›ã€‚"""
    seen = set()
    order: List[int] = []
    for m in _CITE_TAG_RE.finditer(answer_text or ""):
        try:
            n = int(m.group(1))
        except Exception:
            continue
        if 1 <= n <= max_k and n not in seen:
            seen.add(n)
            order.append(n)
    return order

def _citations_by_usage(answer_text: str, contexts: List[Dict]) -> List[Dict]:
    """åªè¿”å›æ­£æ–‡é‡Œå®é™…ä½¿ç”¨åˆ°çš„ [Source n] æ‰€å¯¹åº”çš„ citationsã€‚"""
    from .rag import format_citations
    all_cites = format_citations(contexts)  # é¡ºåºä¸ [Source 1..K] å¯¹åº”
    used = _extract_used_indices(answer_text, len(all_cites))
    return [all_cites[i - 1] for i in used]  # i ä» 1 å¼€å§‹

def _sanitize_inline_citations(text: str, max_k: int) -> str:
    def repl(m):
        n = int(m.group(1))
        return m.group(0) if 1 <= n <= max_k else ""
    return _CITE_TAG_RE.sub(repl, text)

def get_index() -> Index:
    global _index
    if _index is None:
        idx = Index(settings.index_dir)
        idx.load()
        if idx.faiss is None:
            raise HTTPException(status_code=503, detail="Index not built yet. Run /ingest or scripts/ingest.py")
        _index = idx
    return _index

def get_embedder() -> Embedder:
    global _embedder
    if _embedder is None:
        _embedder = Embedder(settings.embedding_model_name, settings.embedding_device)
    return _embedder

@app.get("/healthz")
async def healthz():
    return {"status": "ok"}

@app.post("/ingest", response_model=IngestResponse)
async def ingest(_auth=Depends(require_bearer)):
    docs_count, chunks_count = ingest_corpus(settings.docs_dir, settings.index_dir)
    global _index, _embedder
    _index = None
    _embedder = None
    return IngestResponse(documents_indexed=docs_count, chunks_indexed=chunks_count)

def _extract_answer_text(data: dict) -> str:
    # æ–°å¢ï¼šå¤„ç† "answer" æ˜¯åµŒå¥—å­—ç¬¦ä¸² JSON çš„æƒ…å†µ
    if "answer" in data and isinstance(data["answer"], str):
        try:
            parsed = json.loads(data["answer"].replace("'", '"'))  # å…¼å®¹å•å¼•å·
            if isinstance(parsed, dict) and "response" in parsed:
                return parsed["response"]
        except Exception:
            pass  # fallback below

    try:
        return data["choices"][0]["message"]["content"]
    except Exception:
        pass
    if isinstance(data, dict) and "choices" in data and data["choices"]:
        c0 = data["choices"][0]
        if isinstance(c0, dict) and "text" in c0 and isinstance(c0["text"], str):
            return c0["text"]
    for key in ("content", "answer", "message", "data"):
        v = data.get(key)
        if isinstance(v, str) and v.strip():
            return v
        if isinstance(v, dict):
            for kk in ("content", "text", "answer"):
                vv = v.get(kk)
                if isinstance(vv, str) and vv.strip():
                    return vv
    return str(data)

def _normalize_answer_text(text: str) -> str:
    if not isinstance(text, str):
        return str(text)

    # 1) è§£æ {"response": "..."} æˆ– {'response': '...'}
    try:
        obj = json.loads(text)
        if isinstance(obj, dict) and isinstance(obj.get("response"), str):
            text = obj["response"]
    except Exception:
        m = re.match(r"\s*\{[^}]*'response'\s*:\s*'(.*?)'\s*\}\s*$", text, flags=re.S)
        if m:
            text = m.group(1)

    # 2) å»æ‰æ–‡æœ«å†…åµŒçš„ Sources æ®µ
    text = re.sub(r"\n+Sources\s*\n(?:\[[^\n]+\].*\n?)+\s*$", "", text, flags=re.I)

    # 3) ç»Ÿä¸€æ¢è¡Œï¼Œå¹¶æŠŠå­—é¢é‡ \\n è½¬æˆçœŸæ­£æ¢è¡Œ
    text = text.replace("\r\n", "\n")
    if "\\n" in text:
        text = text.replace("\\n", "\n")

    return text.strip()

def _extract_stream_token_preserve(text_line: str) -> str | None:
    line = text_line
    if not line:
        return None

    # å¤„ç† SSE å‰ç¼€
    s = line.strip()
    if s.lower() == "[done]":
        return None
    if s.startswith("data:"):
        s = s[5:].strip()

    # å°è¯•è§£æ JSONï¼›å¦‚æœä¸æ˜¯ JSONï¼Œç›´æ¥è¿”å›åŸè¡Œï¼ˆä¸æ”¹åŠ¨ï¼‰
    try:
        obj = json.loads(s)
    except Exception:
        return line  # é JSONï¼ŒåŸæ ·è¿”å›ï¼ˆä¿ç•™å…¶ä¸­æ¢è¡Œ/ç©ºæ ¼ï¼‰

    # åœ¨å¸¸è§è·¯å¾„ä¸­å–å­—ç¬¦ä¸²å€¼ï¼ˆåŸæ ·è¿”å›ï¼‰
    def get_path(o, path):
        cur = o
        for p in path:
            if isinstance(p, int):
                if isinstance(cur, list) and len(cur) > p:
                    cur = cur[p]
                else:
                    return None
            else:
                if isinstance(cur, dict) and p in cur:
                    cur = cur[p]
                else:
                    return None
        return cur if isinstance(cur, str) else None

    candidates = [
        ("choices", 0, "delta", "content"),
        ("choices", 0, "text"),
        ("response",),
        ("content",),
        ("answer",),
        ("message", "content"),
        ("data", "content"),
    ]
    for path in candidates:
        val = get_path(obj, path)
        if isinstance(val, str):
            return val  # åŸæ ·è¿”å›ï¼Œä¸åšä»»ä½•æ›¿æ¢æˆ–å»é™¤

    # æ²¡å‘½ä¸­å°±æŠŠæ•´è¡Œ JSON ä¸¢å¼ƒï¼ˆé¿å…å†æŠŠ JSON æ–‡æœ¬å›ä¼ å‰ç«¯ï¼‰
    return None

def _extract_stream_piece(line: str) -> str:
    """
    ä» SmartCare æµå¼æ¯è¡Œé‡Œæå–çº¯æ–‡æœ¬ã€‚
    å…¼å®¹ä»¥ä¸‹æƒ…å†µï¼š
      - {"response": " ... "}
      - {"text": "..."} / {"content": "..."} / {"data": "..."}
      - çº¯æ–‡æœ¬ï¼ˆç›´æ¥è¿”å›ï¼‰
      - SSE é£æ ¼çš„ 'data: {...}'ï¼ˆå¯é€‰å…¼å®¹ï¼‰
    """
    if not line:
        return ""

    # 1) å»æ‰å¯èƒ½çš„ SSE å‰ç¼€
    if line.startswith("data:"):
        line = line[len("data:"):].strip()

    # 2) ä¼˜å…ˆ JSON è§£æ
    try:
        obj = json.loads(line)
        if isinstance(obj, dict):
            for key in ("response", "text", "content", "data"):
                val = obj.get(key)
                if isinstance(val, str):
                    return val
        # å¦‚æœæ˜¯æ•°ç»„æˆ–å…¶ä»–ç»“æ„ï¼Œè¿™é‡Œä¸å¤„ç†
    except Exception:
        pass

    # 3) å…¼å®¹éä¸¥æ ¼ JSONï¼Œç®€å•ç”¨æ­£åˆ™å…œåº•ï¼ˆä¾‹å¦‚ {'response': '...'}ï¼‰
    m = re.search(r"'response'\s*:\s*'(.*?)'", line)
    if m:
        return m.group(1)

    # 4) å¦‚æœ line ä¸æ˜¯ JSONï¼Œå°±å½“ä½œçº¯æ–‡æœ¬ï¼ˆå°‘è§ä½†å®‰å…¨ï¼‰
    #    ä½†è¦æ’é™¤çº¯ç²¹çš„ç©ºç™½/å¿ƒè·³
    if line.strip():
        return line
    return ""

def _not_found_text(lang: str | None) -> str:
    if lang == "zh-Hant":
        return ("æŠ±æ­‰ï¼Œæˆ‘åœ¨ç›®å‰ç´å…¥çš„ç¤¾æœƒç¦åˆ©æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°ä½ é€™å€‹å•é¡Œçš„å…·é«”ç­”æ¡ˆã€‚"
                "ä½ å¯ä»¥å˜—è©¦ï¼š\n"
                "â€¢ æ›ä¸€ç¨®èªªæ³•æˆ–è£œå……æ›´å…·é«”çš„åè©ï¼ˆä¾‹å¦‚æ´¥è²¼åç¨±ã€æœå‹™å–®ä½ï¼‰\n"
                "â€¢ æŒ‡å®šæ–‡ä»¶æˆ–å¹´ä»½ï¼ˆä¾‹å¦‚ 2024 å¹´ LSG Subvention Manualï¼‰\n"
                "å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å¹«ä½ é‡è¿°æŸ¥è©¢æˆ–åˆ—å‡ºç›¸é—œç« ç¯€ä¾›ä½ æŸ¥é–±ã€‚")
    return ("Sorry, I couldn't find a specific answer to this question in the indexed documents.\n"
            "You can try:\n"
            "â€¢ Rephrasing or adding more specific terms (e.g., the exact allowance name/service unit)\n"
            "â€¢ Mentioning a document or year (e.g., 2024 LSG Subvention Manual)\n"
            "If you like, I can help refine your query or show nearby sections.")

from .llm_client import smartcare_chat, smartcare_chat_stream, smartcare_rewrite_query
@app.post("/chat", response_model=ChatAnswer)
async def chat(req: ChatRequest, _auth=Depends(require_bearer)):
    idx = get_index()
    emb = get_embedder()

    # 1) å–å¾— messagesï¼ˆå‰ç«¯ç°åœ¨ä¼šå¸¦æœ€è¿‘è‹¥å¹²è½®ï¼‰
    msgs = [m.dict() if hasattr(m, "dict") else m for m in req.messages]

    # 2) æ”¹å†™ä¸ºç‹¬ç«‹é—®é¢˜ï¼ˆæœ‰å†å²æ—¶æ›´æœ‰æ•ˆï¼‰
    #    å¦‚æœ messages å¾ˆçŸ­ï¼ˆåªæœ‰ä¸€æ¡ userï¼‰ï¼Œå°±ç›´æ¥ç”¨å®ƒ
    if len(msgs) >= 2:
        rewritten = await smartcare_rewrite_query(msgs)
        user_query = rewritten
    else:
        user_query = msgs[-1]["content"]

    # â˜… è¦å‰‡å…œåº•ï¼šè‹¥ä»å«ä»£è©ï¼Œå˜—è©¦è£œä¸Šæœ€è¿‘å¯¦é«”
    if _PRONOUN_PAT.search(user_query):
        ent = _guess_entity_from_history(msgs)
        if ent:
            # ä¸ç²—æš´æ›¿æ¢ç”¨æˆ·åŸå¥ï¼Œåªç»™æ£€ç´¢ä¿¡å·åŠ æ³¨é‡Š
            user_query = f"{user_query} (about {ent})"
    
    # 3) ç”¨æ”¹å†™åçš„ç‹¬ç«‹é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼ˆç¹ä¸­ï¼šåŸæ–‡ + è‹±è¯‘ åŒé€šé“åˆå¹¶ï¼‰
    is_followup_pronoun = bool(_PRONOUN_PAT.search(msgs[-1]["content"]))

    if req.language == "zh-Hant":
        # 3.1 åˆ«å/ä¿—ç§°æ‰©å±•ï¼ˆåªå½±å“æ£€ç´¢ï¼Œä¸æ”¹åŠ¨åŸ messagesï¼‰
        query_zh = _expand_aliases_zh(user_query)

        # 3.2 åŒæ—¶ç”¨ç¹ä¸­ä¸è‹±è¯‘æ£€ç´¢
        try:
            query_en = await smartcare_translate_to_en(query_zh)
        except Exception:
            query_en = None

        hits_zh = hybrid_retrieve(query_zh, idx, emb, settings.top_k, soft=is_followup_pronoun)
        hits_en = hybrid_retrieve(query_en, idx, emb, settings.top_k, soft=True) if query_en else []

        # 3.3 åˆå¹¶å»é‡ï¼ˆä¿ç•™è¾ƒé«˜åˆ†ï¼Œæˆªåˆ° top_kï¼‰
        contexts = _merge_dedup_hits(hits_zh, hits_en, settings.top_k)

        # ï¼ˆå¯é€‰ï¼‰æŠŠâ€œå®é™…ç”¨äºæ£€ç´¢çš„ queryâ€è®°ä¸‹æ¥ä¾¿äºæ—¥å¿—æ’æŸ¥
        user_query = query_zh
    else:
        contexts = hybrid_retrieve(user_query, idx, emb, settings.top_k, soft=is_followup_pronoun)

    # ä»â€œå½“å‰é—®å¥â€å’Œâ€œä¸Šä¸€ä¸ªé—®å¥/å›ç­”â€ä¸­æŠ“ä¸€ä¸ªç„¦ç‚¹çŸ­è¯­
    focus = (_extract_focus_phrase(msgs[-1]["content"]) or
            (len(msgs) >= 2 and _extract_focus_phrase(msgs[-2]["content"])) or
            _guess_entity_from_history(msgs))

    # æŒ‰ç„¦ç‚¹çŸ­è¯­é‡æ’ï¼ˆæŠŠåŒ…å«è¯¥çŸ­è¯­çš„åˆ†ç‰‡å¾€å‰æ¨ï¼‰
    contexts = _boost_by_phrase(contexts, focus, boost=0.35)

    # === å…ˆåˆ¤æ–·æ˜¯å¦æ‰¾å¾—åˆ°ä¾†æº ===
    if len(contexts) < settings.min_sources_required:
        text = _not_found_text(req.language)
        if req.stream:
            async def event_stream():
                yield text
                yield "\nCITATIONS:[]\n"
            return StreamingResponse(event_stream(), media_type="text/plain")
        else:
            return ChatAnswer(answer=text, citations=[])
    
    # === å†æª¢æŸ¥æ˜¯å¦éæ–¼ç± çµ±ï¼ˆä½†å·²æœ‰ä¾†æºï¼‰ ===
    if _should_clarify_smart(user_query) and not _looks_specific(user_query, contexts):
        text = _clarify_question_smart(user_query, req.language, idx, emb)
        if req.stream:
            async def event_stream():
                yield text
                yield "\nCITATIONS:[]\n"
            return StreamingResponse(event_stream(), media_type="text/plain")
        else:
            return ChatAnswer(answer=text, citations=[])

    # 4) æ­£å¸¸æ‹¼ promptï¼ˆæŠŠåŸ messages å‘ç»™æ¨¡å‹ï¼Œè¿™æ ·å®ƒèƒ½â€œæŒ‰ä¸Šä¸‹æ–‡å£å»â€å›ç­”ï¼‰
    prompt_msgs = build_prompt(msgs, contexts)
    if req.language == "zh-Hant":
        prompt_msgs.insert(0, {
            "role": "system",
            "content": (
                "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œ"
                "èªæ°£è¦ªåˆ‡ã€å¥å­ç°¡çŸ­ï¼Œé¿å…ä½¿ç”¨è‰±æ·±è©å½™ï¼Œ"
                "è®“é•·è€…èƒ½å®¹æ˜“æ˜ç™½ã€‚"
            )
        })
    elif req.language == "en":
        prompt_msgs.insert(0, {
            "role": "system",
            "content": (
                "Please answer in clear, short English sentences suitable for older adults. "
                "Avoid jargon and keep the response friendly."
            )
        })

    if req.stream:
        async def event_stream():
            buffer = ""
            async for line in smartcare_chat_stream(prompt_msgs):
                piece = _extract_stream_piece(line)
                if piece:
                    buffer += piece
                    yield piece
            clean_final = _sanitize_inline_citations(buffer, len(contexts))
            cites = _citations_by_usage(clean_final, contexts)
            trailer = "CITATIONS:" + json.dumps(cites, ensure_ascii=False)
            yield "\n" + trailer + "\n"
        return StreamingResponse(event_stream(), media_type="text/plain")

    data = await smartcare_chat(prompt_msgs)
    answer_text = _normalize_answer_text(_extract_answer_text(data))
    answer_text = _sanitize_inline_citations(answer_text, len(contexts))
    citations = _citations_by_usage(answer_text, contexts)
    return ChatAnswer(answer=answer_text, citations=citations)

# from fastapi.responses import PlainTextResponse
# @app.post("/chat/plain", response_class=PlainTextResponse)
# async def chat_plain(req: ChatRequest):
#     idx = get_index()
#     emb = get_embedder()

#     # ========= æ–°å¢ï¼šæ„é€  msgs å¹¶åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ”¹å†™ =========
#     msgs = [m.dict() if hasattr(m, "dict") else m for m in req.messages]

#     if settings.enable_query_rewrite and len(msgs) >= 2:
#         rewritten = await smartcare_rewrite_query(msgs)
#         user_query = rewritten
#     else:
#         user_query = msgs[-1]["content"]

#     # ========= ç„¶åå†è¿›è¡Œæ£€ç´¢ =========
#     is_followup_pronoun = bool(_PRONOUN_PAT.search(msgs[-1]["content"]))
#     contexts = hybrid_retrieve(user_query, idx, emb, settings.top_k)
#     prompt_msgs = build_prompt([m.dict() for m in req.messages], contexts)

#     data = await smartcare_chat(prompt_msgs)
#     answer_text = _extract_answer_text(data)

#     # è§„èŒƒåŒ–è½¬ä¹‰æ¢è¡Œ
#     if "\\n" in answer_text and "\n" not in answer_text:
#         answer_text = answer_text.replace("\\n", "\n")

#     # æŠŠå¼•ç”¨ä¹Ÿé™„åœ¨æ–‡æœ¬æœ«å°¾ï¼ˆé€è¡Œï¼‰
#     cites = format_citations(contexts)
#     lines = [answer_text, "", "References:"]
#     for i, c in enumerate(cites, 1):
#         page = f", page {c['page']}" if c.get("page") is not None else ""
#         lines.append(f"[Source {i}] {c['file']}{page}")

#     return "\n".join(lines)

if __name__ == "__main__":
    uvicorn.run("app.main:app", host=settings.host, port=settings.port, reload=True)

# ============================================================================
# File: app/rag.py
# ============================================================================
import json
import threading
import faiss
import numpy as np
import re
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer

from .security import encrypt_bytes, decrypt_bytes
from .utils import read_markdown_files, infer_page_map

@dataclass
class Chunk:
    text: str
    meta: Dict

class Embedder:
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model = SentenceTransformer(model_name, device=device)
    def encode(self, texts: List[str]) -> np.ndarray:
        return np.array(self.model.encode(texts, normalize_embeddings=True))

class Index:
    def __init__(self, index_dir: str):
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        self.faiss = None
        self.meta: List[Dict] = []
        self.bm25 = None
        self.bm25_corpus_tokens: List[List[str]] = []

    def save(self):
        # faiss index
        if self.faiss is not None:
            raw_path = self.index_dir / "faiss.index"
            faiss.write_index(self.faiss, str(raw_path))
            # ç”¨åŠ å¯†è¦†ç›–å†™å›ï¼›encrypt_bytes å†…éƒ¨ä¼šæ ¹æ® settings.encrypt_data å†³å®šæ˜¯å¦åŠ å¯†/æŠ¥é”™
            data = raw_path.read_bytes()
            raw_path.write_bytes(encrypt_bytes(data))

        # meta.json
        meta_path = self.index_dir / "meta.json"
        meta_bytes = json.dumps(self.meta, ensure_ascii=False).encode("utf-8")
        meta_path.write_bytes(encrypt_bytes(meta_bytes))

        # bm25.json
        if self.bm25 is not None:
            bm25_path = self.index_dir / "bm25.json"
            bm = {"tokens": self.bm25_corpus_tokens}
            bm_bytes = json.dumps(bm, ensure_ascii=False).encode("utf-8")
            bm25_path.write_bytes(encrypt_bytes(bm_bytes))

    def load(self):
        # faiss
        faiss_path = self.index_dir / "faiss.index"
        meta_path = self.index_dir / "meta.json"
        if faiss_path.exists() and meta_path.exists():
            blob = decrypt_bytes(faiss_path.read_bytes())
            tmp = self.index_dir / ".faiss.tmp"
            tmp.write_bytes(blob)
            self.faiss = faiss.read_index(str(tmp))
            tmp.unlink(missing_ok=True)

            mb = decrypt_bytes(meta_path.read_bytes())
            self.meta = json.loads(mb.decode("utf-8"))

        # bm25
        bm25_path = self.index_dir / "bm25.json"
        if bm25_path.exists():
            bb = decrypt_bytes(bm25_path.read_bytes())
            data = json.loads(bb.decode("utf-8"))
            self.bm25_corpus_tokens = data.get("tokens", [])
            if self.bm25_corpus_tokens:
                self.bm25 = BM25Okapi(self.bm25_corpus_tokens)

                
    def build(self, embeddings: np.ndarray, meta: List[Dict], bm25_tokens: Optional[List[List[str]]] = None):
        dim = embeddings.shape[1] if embeddings.size else 384
        index = faiss.IndexFlatIP(dim)
        if embeddings.size:
            index.add(embeddings.astype(np.float32))
        self.faiss = index
        self.meta = meta
        if bm25_tokens:
            self.bm25_corpus_tokens = bm25_tokens
            self.bm25 = BM25Okapi(self.bm25_corpus_tokens)

    def search(self, query_emb: np.ndarray, k: int) -> List[Tuple[int, float]]:
        if self.faiss is None or self.faiss.ntotal == 0:
            return []
        D, I = self.faiss.search(query_emb.astype(np.float32), k)
        return list(zip(I[0].tolist(), D[0].tolist()))

# --- Chunking (safe & fast) ---
def simple_char_chunk(text: str, chunk_size: int, overlap: int) -> List[str]:
    step = max(1, chunk_size - overlap)
    n = len(text)
    if n == 0:
        return []
    return [text[i:i+chunk_size] for i in range(0, n, step)]

# --- Sentence-aware chunking for CJK ---
_SENT_SPLIT = re.compile(r"[ã€‚ï¼ï¼Ÿï¼›ï¼š]\s*")  # ç²—ç²’åº¦åˆ†å¥

def chunk_text(text: str, chunk_size: int, overlap: int) -> list[str]:
    # å…ˆæŒ‰ä¸­æ–‡æ ‡ç‚¹ç²—åˆ†å¥ï¼Œå†åœ¨æ¯ä¸ªå¥æ®µå†…äºŒæ¬¡è£åˆ‡
    parts = []
    segs = [s for s in _SENT_SPLIT.split(text) if s]
    for seg in segs:
        parts.extend(simple_char_chunk(seg, chunk_size, overlap))
    if not segs:  # æ²¡åˆ†å‡ºæ¥å°±é€€å›åŸç­–ç•¥
        parts = simple_char_chunk(text, chunk_size, overlap)
    return parts

# --- Tokenization (CJK-friendly) ---

_CJK = re.compile(r"[\u4e00-\u9fff]")

def _to_halfwidth(s: str) -> str:
    # å…¨è§’è½¬åŠè§’ï¼ˆå¸¸è§ä¸­æ–‡æ•°å­—/æ ‡ç‚¹ï¼‰
    out = []
    for ch in s:
        code = ord(ch)
        if code == 0x3000:  # å…¨è§’ç©ºæ ¼
            code = 0x20
        elif 0xFF01 <= code <= 0xFF5E:
            code -= 0xFEE0
        out.append(chr(code))
    return "".join(out)

def tokenize(text: str) -> list[str]:
    # è½»é‡æ ‡å‡†åŒ–
    text = _to_halfwidth(text)
    if _CJK.search(text):
        # 2-gram + 3-gramï¼Œé€‚é…ç¹ä¸­/ç®€ä¸­
        s = re.sub(r"\s+", "", text)
        toks2 = [s[i:i+2] for i in range(len(s)-1)] if len(s) >= 2 else ([s] if s else [])
        toks3 = [s[i:i+3] for i in range(len(s)-2)] if len(s) >= 3 else []
        return toks2 + toks3
    # è‹±æ–‡/æ•°å­—ï¼šä¿ç•™åŸé€»è¾‘ä½†æ›´ç¨³å¥çš„æ­£åˆ™
    return re.findall(r"[A-Za-z0-9_]+", text.lower())

# --- Ingestion ---
def ingest_corpus(docs_dir: str, index_dir: str) -> Tuple[int, int]:
    docs = read_markdown_files(docs_dir)
    print(f"[ingest] loaded {len(docs)} doc(s)")
    embedder = Embedder(settings.embedding_model_name, settings.embedding_device)

    all_chunks: List[Chunk] = []
    for di, doc in enumerate(docs, 1):
        page_ranges = infer_page_map(doc["text"])  # best-effort
        print(f"[ingest] doc {di}/{len(docs)} -> {len(page_ranges)} page(s) (logical)")
        for pr in page_ranges:
            page_text = doc["text"][pr["start"]:pr["end"]]
            pieces = chunk_text(page_text, settings.chunk_size, settings.chunk_overlap)
            for i, piece in enumerate(pieces):
                meta = {"file": doc["path"], "page": pr["page"], "chunk_id": i, "text": piece}
                all_chunks.append(Chunk(text=piece, meta=meta))
    print(f"[ingest] total chunks: {len(all_chunks)}; start embedding...")

    if not all_chunks:
        index = Index(index_dir)
        index.build(np.zeros((0, 384), dtype=np.float32), [], None)
        index.save()
        return len(docs), 0

    texts = [c.text for c in all_chunks]

    # batch embedding
    BATCH = 512
    emb_list = []
    for i in range(0, len(texts), BATCH):
        emb_list.append(embedder.encode(texts[i:i+BATCH]))
    embeddings = np.vstack(emb_list).astype(np.float32)
    print(f"[ingest] embedding done, shape={embeddings.shape}")

    bm25_tokens = [tokenize(t) for t in texts] if settings.enable_bm25 else None

    meta = [c.meta for c in all_chunks]
    index = Index(index_dir)
    index.build(embeddings, meta, bm25_tokens)
    index.save()
    print("[ingest] index saved")

    return len(docs), len(all_chunks)

# --- Retrieval ---
from .settings import settings

_PENALTY = None           # type: dict | None
_PENALTY_MTIME = 0.0
_PENALTY_LOCK = threading.Lock()

def _load_penalty() -> dict:
    """
    æ‡’åŠ è½½ + mtime å˜æ›´æ—¶é‡è½½ï¼š
    è¿”å› { "file.md::12": 0.20, ... }ï¼Œé”®ä¸º æ–‡ä»¶å::é¡µç ï¼›å€¼ä¸ºæ‰£åˆ†(>=0)ã€‚
    """
    global _PENALTY, _PENALTY_MTIME
    p = Path("data/feedback/penalty.json")
    try:
        mtime = p.stat().st_mtime
    except FileNotFoundError:
        with _PENALTY_LOCK:
            _PENALTY = {}
            _PENALTY_MTIME = 0.0
        return {}

    with _PENALTY_LOCK:
        if _PENALTY is None or mtime != _PENALTY_MTIME:
            try:
                _PENALTY = json.loads(p.read_text(encoding="utf-8"))
            except Exception:
                _PENALTY = {}
            _PENALTY_MTIME = mtime
        return _PENALTY or {}

def hybrid_retrieve(query: str, index: Index, embedder: Embedder, k: int, *, soft: bool=False) -> List[Dict]:
    import re
    _CJK = re.compile(r"[\u4e00-\u9fff]")

    def _tokenize_q(q: str) -> List[str]:
        # è½»é‡è§„èŒƒï¼šå…¨è§’->åŠè§’ï¼Œå‹ç¼©ç©ºç™½
        def _to_halfwidth(s: str) -> str:
            out = []
            for ch in s:
                code = ord(ch)
                if code == 0x3000:  # å…¨è§’ç©ºæ ¼
                    code = 0x20
                elif 0xFF01 <= code <= 0xFF5E:
                    code -= 0xFEE0
                out.append(chr(code))
            return re.sub(r"\s+", " ", "".join(out)).strip()

        s = _to_halfwidth(q)
        if _CJK.search(s):
            s = s.replace(" ", "")
            toks2 = [s[i:i+2] for i in range(len(s)-1)] if len(s) >= 2 else ([s] if s else [])
            toks3 = [s[i:i+3] for i in range(len(s)-2)] if len(s) >= 3 else []
            return toks2 + toks3
        # è‹±æ–‡/æ•°å­—ï¼šå•è¯æ­£åˆ™æ›´ç¨³
        return re.findall(r"[A-Za-z0-9_]+", s.lower())

    is_cjk = bool(_CJK.search(query))
    q_emb = embedder.encode([query])

    # å‘é‡æ£€ç´¢
    vec_hits: List[Tuple[int, float]] = []
    if index.faiss is not None and index.faiss.ntotal > 0:
        D, I = index.faiss.search(q_emb.astype(np.float32), max(k, 50))
        vec_hits = [(int(I[0][i]), float(D[0][i])) for i in range(len(I[0]))]

    # BM25ï¼ˆä¸­æ–‡åˆ†è¯æ”¹é€ ï¼‰
    bm25_hits: List[Tuple[int, float]] = []
    if index.bm25 is not None and index.bm25_corpus_tokens:
        q_tokens = _tokenize_q(query)
        if q_tokens:
            scores = index.bm25.get_scores(q_tokens)
            top_ids = np.argsort(scores)[::-1][:max(k, 50)]
            bm25_hits = [(int(i), float(scores[i])) for i in top_ids]

    # åˆå¹¶åˆ†æ•°ï¼ˆæƒé‡éš CJK è°ƒæ•´ï¼‰
    # ä¸­æ–‡ï¼šBM25 æ›´é‡è¦ï¼›è‹±æ–‡ï¼šå‘é‡ä¸ºä¸»
    alpha_vec = 0.60 if is_cjk else 0.90
    alpha_bm25 = 0.40 if is_cjk else 0.10

    score_map: Dict[int, Dict[str, float]] = {}
    for idx_i, sim in vec_hits:
        m = score_map.setdefault(idx_i, {"vec": -1e9, "bm25": -1e9})
        m["vec"] = max(m["vec"], sim)
    for idx_i, s in bm25_hits:
        m = score_map.setdefault(idx_i, {"vec": -1e9, "bm25": -1e9})
        m["bm25"] = max(m["bm25"], s)

    # è‡ªé€‚åº”é˜ˆå€¼
    vec_thr = settings.min_vec_sim * (0.7 if (soft or is_cjk) else 1.0)
    bm25_thr = settings.min_bm25_score * (0.6 if (soft or is_cjk) else 1.0)

    # ä¹¦åå·çŸ­è¯­ï¼ˆå¦‚ã€Šæ´¥è²¼åŠæœå‹™å”è­°ã€‹ï¼‰ç”¨äºåŠ æƒ
    phrase_boost = 0.35 if is_cjk else 0.20
    m_phrase = re.search(r"ã€Š(.+?)ã€‹", query)
    phrase = m_phrase.group(1).strip() if m_phrase else None

    # è¯»å–ä¸€æ¬¡æƒ©ç½šè¡¨
    _PENALTY = None
    def _load_penalty():
        nonlocal _PENALTY
        if _PENALTY is None:
            from pathlib import Path
            p = Path("data/feedback/penalty.json")
            _PENALTY = json.loads(p.read_text("utf-8")) if p.exists() else {}
        return _PENALTY

    # é˜ˆå€¼è¿‡æ»¤ + èåˆ + phrase åŠ æƒ + æƒ©ç½š
    passed: List[Tuple[int, float]] = []
    pen = _load_penalty()
    for idx_i, sig in score_map.items():
        vec_ok = (sig["vec"] >= vec_thr)
        bm_ok = (sig["bm25"] >= bm25_thr)
        if not (vec_ok or bm_ok):
            continue

        # åŸºç¡€èåˆåˆ†ï¼ˆæ³¨æ„ï¼šFAISS D å·²æ˜¯ä½™å¼¦ï¼ŒBM25 æ˜¯åŸå§‹åˆ†ï¼‰
        combo = alpha_vec * max(sig["vec"], 0.0) + alpha_bm25 * max(sig["bm25"], 0.0)

        # ä¹¦åå·çŸ­è¯­å‘½ä¸­åŠ æƒ
        meta = index.meta[idx_i]
        meta_text = meta.get("text") or (index.texts[idx_i] if hasattr(index, "texts") and index.texts else "")
        if phrase and meta_text and phrase in meta_text:
            combo += phrase_boost

        # åº”ç”¨æƒ©ç½šï¼ˆğŸ‘åé¦ˆï¼‰
        from pathlib import Path
        key = f"{Path(meta['file']).name}::{meta.get('page')}"
        penalty = float(pen.get(key, 0.0))  # ä¾‹å¦‚ 0.15~0.30
        combo -= penalty

        passed.append((idx_i, combo))

    # æ’åº+æˆªæ–­
    passed.sort(key=lambda x: x[1], reverse=True)
    passed = passed[:k]

    # å‡ºç»“æœï¼šè¡¥é½ text å­—æ®µï¼Œä¾¿äºåç»­é€»è¾‘åˆ¤æ–­ä¸æ¸²æŸ“
    results: List[Dict] = []
    for idx_i, combo in passed:
        meta = index.meta[idx_i]
        text = meta.get("text") or (index.texts[idx_i] if hasattr(index, "texts") and index.texts else None)
        results.append({"text": text, "meta": meta, "idx": idx_i, "score": float(combo)})
    return results

# --- Prompt & Citations ---

from pathlib import Path
def build_prompt(messages: List[Dict], contexts: List[Dict]) -> List[Dict]:
    citation_blocks = []
    for i, c in enumerate(contexts, 1):
        file = Path(c["meta"]["file"]).name
        page = c["meta"].get("page")
        ref = f"[Source {i}] {file}" + (f", page {page}" if page is not None else "")
        raw = c["meta"].get("text") or ""
        snippet = raw[:1200]
        citation_blocks.append(f"{ref}\n\n{snippet}")

    K = len(contexts)
    system = {
        "role": "system",
        "content": (
            "You are ElderlyCare HK, a helpful assistant that answers strictly based on the provided Hong Kong Social Welfare Department documents.\n"
            f"- You are given exactly {K} sources. If you cite, you must use ONLY these tokens: "
            + ", ".join(f"[Source {i}]" for i in range(1, K+1)) + ".\n"
            "- Never invent new source indices. If information is not in the sources, clearly say it is not found.\n"
            "- Write plain text only (no JSON). Do NOT include a separate 'Sources' section.\n"
            "- If you cite inline, use the provided tokens verbatim (e.g., ... [Source 1]).\n"
            "- Treat each request as a fresh conversation and DO NOT use any memory beyond the messages provided in this request.\n"
            "- Resolve pronouns in the last user message using the chat history provided in this request.\n"
            "- If the user question is in Traditional Chinese, answer in Traditional Chinese.\n"
            "- If the question is in English, answer in English.\n"
        )
    }
    
    context_msg = {
        "role": "system",
        "content": "Relevant sources:\n\n" + "\n\n---\n\n".join(citation_blocks)
    }
    return [system, context_msg] + messages

def format_citations(contexts: List[Dict]) -> List[Dict]:
    from pathlib import Path
    seen = set()
    out = []
    for c in contexts:
        file = Path(c["meta"]["file"]).name
        page = c["meta"].get("page")
        key = (file, page)
        if key not in seen:
            seen.add(key)
            out.append({"file": file, "page": page, "snippet": None})
    return out

# ============================================================================
# File: app/schemas.py
# ============================================================================
from pydantic import BaseModel
from typing import List, Optional, Literal

class IngestResponse(BaseModel):
    documents_indexed: int
    chunks_indexed: int

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    stream: bool = False
    language: Optional[Literal["en", "zh-Hant"]] = None

class Citation(BaseModel):
    file: str
    page: Optional[int] = None
    snippet: Optional[str] = None

class ChatAnswer(BaseModel):
    answer: str
    citations: List[Citation]

class FeedbackIn(BaseModel):
    threadId: str
    messageId: str
    label: Literal["up", "down"]
    userQuery: str = ""
    answer: str = ""
    language: Optional[Literal["en", "zh-Hant"]] = None
    citations: List[Citation] = []
    # å¯é€‰ï¼šä¸€äº›æ£€ç´¢ä¸å“åº”å…ƒæ•°æ®ï¼Œä¾¿äºåˆ†æ/è‡ªåŠ¨è°ƒå‚
    meta: Optional[dict] = None

# ============================================================================
# File: app/security.py
# ============================================================================
import base64, os
from fastapi import HTTPException, Request
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from .settings import settings

# --------- è®¿é—®æ§åˆ¶ ----------
def require_bearer(request: Request):
    if not settings.require_auth:
        return
    auth = request.headers.get("Authorization", "")
    if not auth.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing bearer token")
    token = auth.split(" ", 1)[1].strip()
    expected = (settings.api_bearer_token or "").strip()
    if not expected or token != expected:
        raise HTTPException(status_code=403, detail="Invalid token")

# --------- åŠ è§£å¯† ----------
def _get_aesgcm():
    key_b64 = (settings.encryption_key_b64 or "").strip()
    if not key_b64:
        return None
    key = base64.urlsafe_b64decode(key_b64)
    return AESGCM(key)
        
def encrypt_bytes(plain: bytes) -> bytes:
    from secrets import token_bytes
    aes = _get_aesgcm()
    if settings.encrypt_data and aes is None:
        raise RuntimeError("ENCRYPT_DATA=true but no ENCRYPTION_KEY_B64 provided")
    if aes is None:
        return plain
    nonce = token_bytes(12)
    return nonce + aes.encrypt(nonce, plain, b"")

def decrypt_bytes(blob: bytes) -> bytes:
    aes = _get_aesgcm()
    if settings.encrypt_data and aes is None:
        raise RuntimeError("ENCRYPT_DATA=true but no ENCRYPTION_KEY_B64 provided")
    if aes is None:
        return blob
    if len(blob) < 13:
        raise ValueError("ciphertext too short")
    nonce, ct = blob[:12], blob[12:]
    return aes.decrypt(nonce, ct, b"")

# ============================================================================
# File: app/settings.py
# ============================================================================
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field
from typing import List, Optional

class Settings(BaseSettings):
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    debug: bool = True

    # Paths
    docs_dir: str = "data/docs"
    index_dir: str = "data/index"

    # RAG chunking
    chunk_size: int = 1500
    chunk_overlap: int = 200
    top_k: int = 5

    # Embeddings
    embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
    embedding_device: str = "cpu"  # set to "cuda" if available

    # HKUST LLM API
    smartcare_base_url: str = "https://smartlab.cse.ust.hk/smartcare/dev/llm_chat/"
    temperature: float = 0.0
    max_tokens: int = 1024

    # Feature flags
    enable_bm25: bool = True
    enable_query_rewrite: bool = True

    # Retrieval gating
    min_vec_sim: float = 0.4    # ä½™å¼¦ç›¸ä¼¼åº¦(Inner Product, å½’ä¸€åŒ–å)
    min_bm25_score: float = 5   # BM25 æœ€å°åˆ†ï¼ˆ0~å‡ åï¼Œè¯­æ–™è€Œå®šï¼‰
    min_sources_required: int = 1  # éœ€è¦è‡³å°‘ N ä¸ªå‘½ä¸­çš„æ¥æºæ‰è®¤ä¸ºâ€œæ‰¾åˆ°äº†â€

    # Security
    api_bearer_token: Optional[str] = None
    encryption_key_b64: Optional[str] = None
    allowed_origins: list[str] = ["http://localhost:5173/"]  # ç”Ÿäº§å»ºè®®ç™½åå•
    require_auth: bool = True  # æ˜¯å¦å¼ºåˆ¶ Bearer
    encrypt_data: bool = True  # æ˜¯å¦å¯¹æ•°æ®è½ç›˜åŠ å¯†

    # Pydantic v2 çš„é…ç½®å†™æ³•ï¼ˆå¿½ç•¥é¢å¤–ç¯å¢ƒå˜é‡ï¼›è¯»å– .envï¼‰
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

settings = Settings()

# ============================================================================
# File: app/utils.py
# ============================================================================
import re
from pathlib import Path
from typing import List, Dict

# â€”â€” æ–°ï¼šæ›´é²æ£’çš„å¤šæ ·å¼é¡µç åŒ¹é… â€”â€” 
# è¯´æ˜ï¼š
#  - æ¯ä¸ªæ ·å¼åªæœ‰ä¸€ä¸ªæ•è·ç»„æ˜¯é¡µç æ•°å­—ï¼›ä¸‹é¢çš„ infer_page_map ä¼šæ‰¾å‡ºå‘½ä¸­çš„é‚£ä¸ªåˆ†ç»„
_PAGE_PATTERNS = [
    r"(?:^|\n)\s*Page\s*(\d+)\s*(?:\n|$)",            # Page 12
    r"(?:^|\n)\s*p\.\s*(\d+)\s*(?:\n|$)",             # p. 12
    r"(?:^|\n)\s*é \s*(\d+)\s*(?:\n|$)",              # é  12
    r"(?:^|\n)\s*ç¬¬\s*(\d+)\s*é \s*(?:\n|$)",          # ç¬¬ 12 é 
    r"(?:^|\n)\s*Page\s*(\d+)\s*of\s*\d+\s*(?:\n|$)", # Page 12 of 200
    # â€”â€” HTML / PDF è½¬ Markdown å¸¸è§é”šç‚¹ â€”â€” 
    r"<span[^>]*\bid=['\"]page-(\d+)(?:-[^'\"]+)?['\"][^>]*>",   # <span id="page-30-0">
    r"<a[^>]*\b(?:id|name)=['\"]page-(\d+)['\"][^>]*>",          # <a id="page-30"> / <a name="page-30">
    r"<div[^>]*\bclass=['\"][^'\"]*\bpage\b[^'\"]*['\"][^>]*\bdata-page=['\"](\d+)['\"][^>]*>",  # <div class="page" data-page="30">
]
_PAGE_RE = re.compile("|".join(f"(?:{p})" for p in _PAGE_PATTERNS), re.IGNORECASE)

def read_markdown_files(docs_dir: str) -> List[Dict]:
    paths = list(Path(docs_dir).glob("**/*.md")) + list(Path(docs_dir).glob("**/*.markdown"))
    docs = []
    for p in paths:
        try:
            text = p.read_text(encoding="utf-8", errors="ignore")
            docs.append({"path": str(p), "text": text})
        except Exception as e:
            print(f"[WARN] Failed to read {p}: {e}")
    return docs

def infer_page_map(md_text: str) -> List[Dict]:
    matches = list(_PAGE_RE.finditer(md_text))
    if not matches:
        return [{"page": None, "start": 0, "end": len(md_text)}]

    def first_int_group(m: re.Match) -> int | None:
        # ç”±äºæˆ‘ä»¬æ˜¯ä¸€ä¸ªå¤§ ORï¼Œgroups() é‡Œåªæœ‰ä¸€ä¸ªæ˜¯æ•°å­—ï¼Œå…¶å®ƒæ˜¯ None
        for g in m.groups():
            if g and g.isdigit():
                return int(g)
        return None

    pages = []
    for i, m in enumerate(matches):
        page_no = first_int_group(m)
        if page_no is None:
            # ç†è®ºä¸ä¼šå‘ç”Ÿï¼›é˜²å¾¡æ€§å¤„ç†
            continue
        start = m.start()
        end = matches[i+1].start() if i+1 < len(matches) else len(md_text)
        pages.append({"page": page_no, "start": start, "end": end})
    return pages

# ============================================================================
# File: scripts/build_penalty.py
# ============================================================================
from pathlib import Path
import json, collections

# è®©è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œè€Œä¸æ˜¯å½“å‰è„šæœ¬æ–‡ä»¶å¤¹
ROOT = Path(__file__).resolve().parent.parent
FB = ROOT / "data/feedback/feedback.jsonl"
OUT = ROOT / "data/feedback/penalty.json"

cnt = collections.Counter()
if not FB.exists():
    print(f"[WARN] Feedback file not found: {FB}")
    raise SystemExit(0)

for line in FB.read_text(encoding="utf-8").splitlines():
    if not line.strip():
        continue
    try:
        r = json.loads(line)
    except Exception as e:
        print(f"[WARN] bad line: {e}")
        continue
    if r.get("label") != "down":
        continue
    for c in r.get("citations", []):
        key = (c.get("file"), c.get("page"))
        cnt[key] += 1

# é˜ˆå€¼ï¼š>=3 æ¬¡ down è®°å…¥æƒ©ç½š
pen = {f"{k[0]}::{k[1]}": min(1.0, 0.15 + 0.05*v) for k,v in cnt.items() if v >= 3}
OUT.parent.mkdir(parents=True, exist_ok=True)
OUT.write_text(json.dumps(pen, ensure_ascii=False, indent=2), encoding="utf-8")
print(f"[OK] Penalized pages: {len(pen)} â†’ {OUT}")

# ============================================================================
# File: scripts/ingest.py
# ============================================================================
from app.rag import ingest_corpus
from app.settings import settings

if __name__ == "__main__":
    docs, chunks = ingest_corpus(settings.docs_dir, settings.index_dir)
    print(f"Indexed documents: {docs}, chunks: {chunks}")

# ============================================================================
# File: scripts/tune_thresholds.py
# ============================================================================
import json, statistics, os
from pathlib import Path

FB = Path("data/feedback/feedback.jsonl")
ENV = Path(".env")

def load_feedback():
    if not FB.exists(): return []
    for line in FB.read_text(encoding="utf-8").splitlines():
        try: yield json.loads(line)
        except: pass

def window(feeds, days=7):
    # ç®€åŒ–ï¼šä¸æŒ‰æ—¶é—´çª—å£ä¹Ÿå¯ï¼Œå…ˆå…¨é‡
    return list(feeds)

def compute_uprate(items):
    up = sum(1 for x in items if x.get("label")=="up")
    down = sum(1 for x in items if x.get("label")=="down")
    return up / max(1, up+down)

def rewrite_env(**kv):
    old = ENV.read_text(encoding="utf-8") if ENV.exists() else ""
    for k,v in kv.items():
        if f"{k}=" in old:
            old = re.sub(rf"^{k}=.*$", f"{k}={v}", old, flags=re.M)
        else:
            old += f"\n{k}={v}"
    ENV.write_text(old, encoding="utf-8")

if __name__ == "__main__":
    data = list(window(load_feedback(), days=7))
    rate = compute_uprate(data)
    # è¯»å–å½“å‰é˜ˆå€¼ï¼ˆä¹Ÿå¯ç”¨ settingsï¼‰
    min_vec = float(os.getenv("MIN_VEC_SIM", "0.40"))
    min_bm25 = float(os.getenv("MIN_BM25_SCORE", "5"))
    # ç®€å•ç­–ç•¥
    if rate < 0.7:
        min_vec = min(min_vec + 0.02, 0.60)
        min_bm25 = min_bm25 + 0.5
    elif rate > 0.8:
        min_vec = max(min_vec - 0.02, 0.30)
        min_bm25 = max(min_bm25 - 0.5, 3.0)
    # å†™å› .envï¼ˆä¸‹æ¬¡è¿›ç¨‹é‡å¯æˆ–çƒ­åŠ è½½è¯»å–ï¼‰
    rewrite_env(MIN_VEC_SIM=min_vec, MIN_BM25_SCORE=min_bm25)
    print({"up_rate": rate, "MIN_VEC_SIM": min_vec, "MIN_BM25_SCORE": min_bm25})

# ============================================================================
# File: requirements.txt
# ============================================================================
# FastAPI & server
fastapi==0.115.2
uvicorn[standard]==0.30.6
httpx==0.27.2
cryptography==43.0.3

# RAG
faiss-cpu==1.8.0.post1
rank-bm25==0.2.2
sentence-transformers==3.0.1
numpy==1.26.4
pydantic==2.9.2
pydantic-settings==2.6.1

# ============================================================================
# File: README.md
# ============================================================================
# ElderlyCare HK â€” Backend (FastAPI + RAG)

## 1) Setup
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
mkdir -p data/docs data/index
# Put your Markdown files under data/docs/
```

Create a `.env` (optional):
```
# Server
HOST=0.0.0.0
PORT=8000
DEBUG=true

# RAG
CHUNK_SIZE=1500
CHUNK_OVERLAP=200
TOP_K=5
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu

# SmartCare
SMARTCARE_BASE_URL=https://smartlab.cse.ust.hk/smartcare/dev/llm_chat/
TEMPERATURE=0.0
MAX_TOKENS=1024
ENABLE_BM25=true
```

## 2) Build the index
```bash
python -m scripts.ingest
```
This creates FAISS index + metadata under `data/index/`.

## 3) Run the server
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8001
```

## 4) Call the API

> If the Server and the Client are not on the same machine, then it is required ```ip a | grep 'inet '``` in bash to get the IP address of server.

### Health
```bash
curl http://localhost:8001/healthz
```

### Chat (non-stream)
```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role":"user","content":"What about Eligibility for Operating Subvented Welfare?"}
    ],
    "stream": false
  }' -w '\n'
```

### Stream (raw lines)
```bash
curl -N -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role":"user","content":"Eligibility for Old Age Living Allowance?"}
    ],
    "stream": true
  }' -w '\n'
```

## Notes
- You should better run ```scripts/tune_thresholds.py``` once per day to improve the performance of chat robot.
- Citations list the file name and page if detectable from markdown. If your MD lacks page markers, the backend still cites the file.
- For better page/snippet support, extend `infer_page_map` to parse your MD structure (e.g., headings) and store character spans for snippets.
- To switch to GPU embeddings, set `EMBEDDING_DEVICE=cuda` and ensure CUDA is available.
- You can later add caching, rate limiting, and feedback logging.
